@startmindmap

title =<i><b><u>Neural Network Architecture \n<color #red><i>Activation Functions (or neurons) both words can be used interchangeably
* <b>[[https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&ab_channel=3Blue1Brown But what is a neural network? | Chapter 1, Deep learning]]
** <b>[[https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2&ab_channel=3Blue1Brown Gradient descent, how neural networks learn | Chapter 2, Deep learning]]
*** <b>[[https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&ab_channel=3Blue1Brown What is backpropagation really doing? | Chapter 3, Deep learning]]
**** <b>[[https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4&ab_channel=3Blue1Brown Backpropagation calculus | Chapter 4, Deep learning]]
**** <b>[[https://www.youtube.com/watch?v=YG15m2VwSjA&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=5&ab_channel=3Blue1Brown Visualizing the chain rule and product rule | Chapter 4, Essence of calculus]]

* <b>[[https://www.youtube.com/watch?v=dPWYUELwIdM&ab_channel=freeCodeCamp.org How Deep Neural Networks Work - Full Course for Beginners]]
** <b>[[https://www.youtube.com/watch?v=ILsA4nyG7I0&list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp&ab_channel=BrandonRohrer How Deep Neural Networks Work]]
*** <b>[[https://www.youtube.com/watch?v=TkwXa7Cvfr8&list=PL_UEf8P1IjTjsbPasIQf3jWfQnM0xt0ZN&index=4&ab_channel=EmergentGarden Watching Neural Networks Learn]]

!theme hacker

*[#darkblue] <i>Functions are
**[#lightblue] <i><size:14>Input-Output Machines\n<img:images/img.png>
***[#green] <b><i><size:14><color #white>Input (x) Set of Numbers
***[#green] <b><i><size:14><color #white>Output (y) Set of Numbers
***[#green] <b><i><size:14><color #white>function (f or sin etc.) defines Relationship Between Numbers (x and y)
****[#yellow] <i><size:14><color #black>Here "function" is "sin" & numbers are (x & y)\n<img:images/img_1.png>

*[#darkblue] <i>Neural Network's Role is
**[#lightblue] <i><size:14>To "approximate" Unknown "Function"
***[#green] <b><i><size:14><color #white>Based on given
****[#orange] <color #black><b><i><size:14>"Data Points" from "unknown Function"\n<img:images/img_2.png>
*****[#yellow] <i><size:14>This is our "Data Set", <b><i>The Inputs & Outputs
***[#green] <b><i><size:14><color #white>Accurately Predict Outputs
****[#orange] <color #black><b><i><size:14>Curve Fitting
*****[#yellow] <i><size:14>Approximating a function that fits these "data points" that allows us \n<i><size:14>to accurately predict "outputs" given inputs that are not in our data set\n<img:images/img_3.png>
****[#orange] <color #black><b><i><size:14>Generalizable Process
*****[#yellow] <i><size:14>Meaning can be applied to any "data-set"
******[#yellow] <i><size:14>structured, unstructured, audio, video, images etc.
****[#orange] <color #black><b><i><size:14>Construct Any Function
**[#lightblue] <i><size:14>"Neural Network" itself is also a "function" & should \napproximate some unknown target functions\n<img:images/img_4.png>
***[#green] <b><i><size:14><color #white>Fully Connected Feed Forward Network\n<img:images/img_5.png>
****[#orange] <color #black><b><i><size:14>Inputs and Outputs
*****[#yellow] <i><size:14>Features and Predictions
*****[#yellow] <i><size:14>May take form of vectors, arrays of numbers
****[#orange] <color #black><b><i><size:14>Neurons\n<img:images/img_6.png>
*****[#yellow] <i><size:14>Many Inputs, One Output
****[#orange] <color #black><b><i><size:14>Weighted Sum and Bias
*****[#yellow] <i><size:14>Each "input" is multiplied by its own "weight" and "added" up \n<i><size:14>along with "one" extra "weight" (w4) called a "bias" \n<img:images/img_7.png>
******[#yellow] <i><size:14>Rewrite this "weighted sum" with "linear algebra", we can put \n<i><size:14>our "inputs" into a "vector" with an "extra one" for the "bias" \n<i><size:14>& our "weights" into another "vector" &then take "dot product"\n<img:images/img_8.png> <img:images/img_9.png>
****[#orange] <color #black><b><i><size:14>Activation Function
*****[#yellow] <i><size:14>ReLU
******[#yellow] <i><size:14>The "result" of "dot product" is passed to (ReLU)\n<img:images/img_10.png>
*******[#yellow] <i><size:14>returns \t<b><size:32>0
***[#green] <b><i><size:14><color #white>Learning Process
****[#orange] <color #black><b><i><size:14>Minimize Error or Loss
****[#orange] <color #black><b><i><size:14>Back Propagation


*[#darkblue] <i>Neural Network Architecture
**[#lightblue] <i>Basics of Neural Networks
***[#yellow] <i><size:14>**Neurons and Activation Functions -** Neurons simulate the functioning of biological neurons, and activation functions determine the output of a neuron.\n*<size:14><i>"Neurons process input data" and "activation functions introduce non-linearity."
***[#darkorange] <i><size:16>**[[Weights-and-Biases.puml Weights and Biases -]]** Weights control the strength of connections, and biases introduce a shift in neuron behavior. \n*<size:14><i>Neural networks "learn" by adjusting "weights and biases" in each "layer".
***[#yellow] <i><size:14>**Layers **(Input, Hidden, Output)** -** Neural networks are organized into **layers (input, hidden, and output).** \n* <size:14><i>"Information" flow through **layers (input, hidden, and output)** in a "network".
***[#darkorange] <i><size:14>**Feedforward and [[Backpropagation.puml Backpropagation]] -** Feedforward processes data through the network, while backpropagation adjusts weights during training. \n* <color #black><size:14><i>Feedforward for **"prediction"** and backpropagation for <b>"learning".

**[#lightblue] <i><size:18>[[Neural-Network-Types.puml Types of Neural Networks]]
***[#yellow] <i><size:14>**1- Feedforward Neural Networks **(FNN)** -** Standard neural networks where information flows in one direction. \n* <size:14><i>Basic "forward flow" from "input to output".
***[#yellow] <i><size:14>**2- Convolutional Neural Networks **(CNN)** -** Specialized for "image processing", using convolutional layers. \n* <size:14><i>Specialized for "image data" with convolutional layers.
***[#yellow] <i><size:14>**3- Recurrent Neural Networks **(RNN)** -** Suitable for sequential data, with "recurrent connections". \n* <size:14><i>Process "sequential data" with "loops".
***[#yellow] <i><size:14>**4- Long Short-Term Memory **(LSTM)** -** A type of RNN with improved memory capabilities. \n* <size:14><i>Handles "long-term dependencies" in "sequences".
***[#yellow] <i><size:14>**5- Gated Recurrent Units **(GRU)** -** A "simpler RNN" variant with "gating mechanisms".
***[#yellow] <i><size:14>**6- And many more**

**[#lightblue] <i>Layer Types
***[#yellow] <i><size:14>**Input Layer -** Receives and preprocesses input data and then, passes it to the hidden layers.
***[#yellow] <i><size:14><color #red>**Hidden Layers -** Process information via various types, such as **fully connected, convolutional, or recurrent.**
****[#white] <i><size:14>Fully Connected **(Dense)** Layers **- Each neuron connected to all neurons in the previous layer.**
****[#white] <i><size:14>Convolutional Layers **- Detect patterns in input data using filters.**
****[#white] <i><size:14>Recurrent Layers **- Process sequences and maintain hidden states.**
***[#yellow] <i><size:14>**Output Layer -** Produces the final network output.

**[#lightblue] <i><size:18>[[Activation-Functions.puml Activation Functions]]
***[#yellow] <i><size:14>**Sigmoid -** <color #red>S-shaped curve used in <b>earlier (1980's) neural networks. \n* <size:14><i>Smoothly maps input to the range **(0, 1).**
***[#yellow] <i><size:14>**Hyperbolic Tangent (tanh) -** <color #red>Similar to the **sigmoid** but with output in the range **[-1, 1].**
***[#yellow] <i><size:18>**Rectified Linear Unit (ReLU) -** <color #red>Widely used activation function with a "linear segment", "output" is **max(0, input).**
***[#yellow] <i><size:18>**Leaky ReLU (LReLU) -** <color #red>Variation of ReLU with a small slope for negative values. Avoids dead neurons by having a small slope for negative input.
***[#yellow] <i><size:14>**Exponential Linear Unit (ELU) -** <color #red>An activation function that avoids the vanishing gradient problem. Smoothly handles both positive and negative values.

**[#lightblue] <size:18><i>[[Loss-Functions.puml Loss Functions]]
***[#yellow] <i><size:14>**Mean Squared Error (MSE) -** Commonly used for **"regression"** tasks. Measures the "average squared difference" between "predicted" and "actual" values.
***[#yellow] <i><size:14>**Cross-Entropy Loss -** Ideal for **"classification"** problems. Measures the "dissimilarity" between "predicted and actual" "distributions".
***[#yellow] <i><size:14>**Hinge Loss -** Used in **"support vector machines"** and **"deep learning"**.
***[#yellow] <i><size:14>**Huber Loss -** A combination of **"MSE"** and **"absolute error loss"**.
***[#yellow] <i><size:14>**Custom Loss Functions -** Tailored for specific **tasks or objectives or problems.**

**[#lightblue] <size:18><i>[[Optimizers-in-Neural-Networks.puml Optimizers]]
***[#yellow] <i><size:14>**Gradient Descent -** Iterative algorithm for minimizing the loss function. Updates weights based on the gradient of the loss.
***[#yellow] <i><size:14>**Stochastic Gradient Descent (SGD) -** A "variation of gradient descent" that processes small "data batches". Uses "random samples" for faster "convergence".
***[#yellow] <i><size:14>**Adam Optimizer -** Popular adaptive "learning rate" optimizer. Adaptive optimization with "momentum".
***[#yellow] <i><size:14>**RMSprop -** Another adaptive "learning rate" **"algorithm"**. "Root Mean Square Propagation" for adjusting "learning rates".
***[#yellow] <i><size:14>**Adagrad -** Optimizer that adapts "learning rates" for each "parameter". Adaptive "learning rates" for "sparse data".

**[#lightblue] <size:18><i>[[Regularization-in-Neural-Networks.puml Regularization]]
***[#yellow] <i><size:14>**L1 Regularization (Lasso) -** Encourages "sparsity" in "weight parameters".
***[#yellow] <i><size:14>**L2 Regularization (Ridge) -** Encourages "small weights" and prevents "overfitting". Prevents "large weight" values.
***[#darkorange] <i><size:14>**[[Dropout-in-Neural-Networks.puml Dropout -]]** Technique to prevent "overfitting" by randomly deactivating "neurons". Randomly deactivates "neurons" during "training".
***[#yellow] <i><size:14>**Batch Normalization -** Normalizes input to hidden layers, improving "training stability". Normalizes "activations" in "mini-batches".
***[#yellow] <i><size:14>**Weight Decay -** Reduces the size of "weights" during "training". "Penalizes" "large weight" values.

**[#lightblue] <i>Training and Validation
***[#yellow] <i><size:14>**Training Data and Batches -** The data used for training, divided into batches. Data used for "training" and "splitting it into batches".
***[#yellow] <i><size:14>**Learning Rate -** Controls the **"step size"** during "optimization". Controls the "step size" in "weight updates".
***[#yellow] <i><size:14>**Early Stopping -** Prevents "overfitting" by monitoring "validation loss". Prevents "overfitting" by "stopping training" when "validation error increases".
***[#yellow] <i><size:14>**Overfitting and Underfitting -** Challenges in "model" "fitting". Challenges in "finding" the "right model complexity".
***[#yellow] <i><size:14>**Validation and Testing -** Ensures model generalization and evaluation. Crucial for "assessing" "model performance".

**[#lightblue] <i>Applications
***[#yellow] <i><size:14>**Image Classification -** Identifying objects or patterns in images.
***[#yellow] <i><size:14>**Natural Language Processing (NLP) -** Processing and understanding text data.
***[#yellow] <i><size:14>**Speech Recognition -** Converting spoken language into text.
***[#yellow] <i><size:14>**Computer Vision -** Processing and understanding visual data. Analyzing visual data from images and videos.
***[#yellow] <i><size:14>**Autonomous Vehicles -** Enabling self-driving cars and robotics.

**[#lightblue] <i>Frameworks and Libraries
***[#yellow] <i><size:14>**TensorFlow -** An **open-source** and versatile **"machine learning framework"** by **Google**.
***[#yellow] <i><size:14>**PyTorch -** An open-source **"deep learning platform"** by **Facebook**. Platform for research and development.
***[#yellow] <i><size:14>**Keras -** A high-level neural networks API for rapid prototyping.
***[#yellow] <i><size:14>**scikit-learn -** A machine learning library for Python.
***[#yellow] <i><size:14>**Caffe -** A "deep learning framework" **for vision tasks** developed by the "Berkeley Vision and Learning Center".

**[#lightblue] <i>Challenges and Advanced Concepts
***[#yellow] <i><size:14>**Vanishing and Exploding Gradients -** Issues with gradients during training.
***[#yellow] <i><size:14>**Gradient Clipping -** Technique to mitigate gradient issues.
***[#yellow] <i><size:14>**Transfer Learning -** Leveraging pre-trained models for new tasks.
***[#yellow] <i><size:14>**Generative Adversarial Networks (GANs) -** Models for generating data.
***[#yellow] <i><size:14>**Reinforcement Learning with Neural Networks -** Integrating neural networks into reinforcement learning.

**[#lightblue] <i>Interdisciplinary Fields
***[#yellow] <i><size:14>**Artificial Intelligence (AI) -** "Neural networks" are a "key component" of AI. "Neural networks" play a "central role" in AI.
***[#yellow] <i><size:14>**Machine Learning (ML) -** A subset of AI that heavily relies on neural networks.
***[#yellow] <i><size:14>**Deep Learning -** A subfield of ML focused on neural networks.
***[#yellow] <i><size:14>**Cognitive Computing -** Emulating human-like thinking and reasoning.
***[#yellow] <i><size:18><color #red>**Neural Network Hardware -** Specialized hardware for efficient neural network execution.

**[#lightblue] <i>Ethical and Legal Considerations
***[#darkorange] <i><size:14>**[[Biases-and-Bias-Neurons.puml Bias and Fairness -]]** Addressing biases in training data and models.
***[#yellow] <i><size:14>**Privacy Concerns -** Protecting sensitive data in AI applications.
***[#yellow] <i><size:14>**Regulatory Compliance -** Adhering to legal and ethical standards.
***[#yellow] <i><size:14>**Responsible AI -** Ensuring AI systems make ethical decisions.

**[#lightblue] <i>Future Developments
***[#yellow] <i><size:14>**Architectural Innovations -** Ongoing advancements in "neural network design".
***[#yellow] <i><size:18><color #red>**Explainable AI (XAI) -** Making "AI decision-making" more "transparent".
***[#yellow] <i><size:14>**Human-Machine Collaboration -** Synergy between AI and human intelligence.
***[#yellow] <i><size:14>**Integration with IoT -** Neural networks in the Internet of Things **(IoT)**.
***[#yellow] <i><size:14>**Quantum Computing and Neural Networks -** Potential synergy between "quantum computing" and "deep learning".

**[#lightblue] <i>Research Areas and Unsolved Problems
***[#yellow] <i><size:14>**Explainability in Deep Learning -** Making neural network decisions "understandable".
***[#yellow] <i><size:14>**Compositional and Symbolic Reasoning -** Enhancing AI's "reasoning capabilities".
***[#yellow] <i><size:14>**AI Safety and Robustness -** Ensuring AI systems are safe and reliable.
***[#yellow] <i><size:14>**Generalization in Neural Networks -** Understanding and improving **"generalization".**
***[#yellow] <i><size:14>**Conscious AI and Cognitive Science -** Exploring AI with "human-like cognitive abilities".
@endmindmap
