@startmindmap
title =<i><b><u>Neural Network Architecture \n<color #red><i>Activation Functions (or neurons) both words can be used interchangeably
* <b>[[https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&ab_channel=3Blue1Brown But what is a neural network? | Chapter 1, Deep learning]]
** <b>[[https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2&ab_channel=3Blue1Brown Gradient descent, how neural networks learn | Chapter 2, Deep learning]]
*** <b>[[https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&ab_channel=3Blue1Brown What is backpropagation really doing? | Chapter 3, Deep learning]]
**** <b>[[https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4&ab_channel=3Blue1Brown Backpropagation calculus | Chapter 4, Deep learning]]
**** <b>[[https://www.youtube.com/watch?v=YG15m2VwSjA&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=5&ab_channel=3Blue1Brown Visualizing the chain rule and product rule | Chapter 4, Essence of calculus]]

* <b>[[https://www.youtube.com/watch?v=dPWYUELwIdM&ab_channel=freeCodeCamp.org How Deep Neural Networks Work - Full Course for Beginners]]
** <b>[[https://www.youtube.com/watch?v=ILsA4nyG7I0&list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp&ab_channel=BrandonRohrer How Deep Neural Networks Work]]
*** <b>[[https://www.youtube.com/playlist?list=PL_UEf8P1IjTjsbPasIQf3jWfQnM0xt0ZN Neural network learns the mandelbrot set]]

!theme hacker
*[#darkblue] <i>Neural Network Architecture
**[#lightblue] <i>Basics of Neural Networks
***[#yellow] <i><size:14>**Neurons and Activation Functions -** Neurons simulate the functioning of biological neurons, and activation functions determine the output of a neuron.\n*<size:14><i>"Neurons process input data" and "activation functions introduce non-linearity."
***[#yellow] <i><size:14>**Weights and Biases -** Weights control the strength of connections, and biases introduce a shift in neuron behavior. \n*<size:14><i>Neural networks "learn" by adjusting weights and biases in each layer.
***[#yellow] <i><size:14>**Layers **(Input, Hidden, Output)** -** Neural networks are organized into **layers (input, hidden, and output).** \n* <size:14><i>"Information" flow through **layers (input, hidden, and output)** in a "network".
***[#yellow] <i><size:14>**Feedforward and Backpropagation -** Feedforward processes data through the network, while backpropagation adjusts weights during training. \n* <size:14><i>Feedforward for "prediction" and backpropagation for "learning".

**[#lightblue] <i><size:14>Types of Neural Networks
***[#yellow] <i><size:14>**1- Feedforward Neural Networks **(FNN)** -** Standard neural networks where information flows in one direction. \n* <size:14><i>Basic "forward flow" from "input to output".
***[#yellow] <i><size:14>**2- Convolutional Neural Networks **(CNN)** -** Specialized for "image processing", using convolutional layers. \n* <size:14><i>Specialized for "image data" with convolutional layers.
***[#yellow] <i><size:14>**3- Recurrent Neural Networks **(RNN)** -** Suitable for sequential data, with "recurrent connections". \n* <size:14><i>Process "sequential data" with "loops".
***[#yellow] <i><size:14>**4- Long Short-Term Memory **(LSTM)** -** A type of RNN with improved memory capabilities. \n* <size:14><i>Handles "long-term dependencies" in "sequences".
***[#yellow] <i><size:14>**5- Gated Recurrent Units **(GRU)** -** A "simpler RNN" variant with "gating mechanisms".
***[#yellow] <i><size:14>**6- And many more**

**[#lightblue] <i>Layer Types
***[#yellow] <i><size:14>**Input Layer -** Receives and preprocesses input data and then, passes it to the hidden layers.
***[#yellow] <i><size:14><color #red>**Hidden Layers -** Process information via various types, such as **fully connected, convolutional, or recurrent.**
****[#white] <i><size:14>Fully Connected **(Dense)** Layers **- Each neuron connected to all neurons in the previous layer.**
****[#white] <i><size:14>Convolutional Layers **- Detect patterns in input data using filters.**
****[#white] <i><size:14>Recurrent Layers **- Process sequences and maintain hidden states.**
***[#yellow] <i><size:14>**Output Layer -** Produces the final network output.

**[#lightblue] <i><size:22>Activation Functions
***[#yellow] <i><size:14>**Sigmoid -** <color #red>S-shaped curve used in <b>earlier (1980's) neural networks. \n* <size:14><i>Smoothly maps input to the range **(0, 1).**
***[#yellow] <i><size:14>**Hyperbolic Tangent (tanh) -** <color #red>Similar to the **sigmoid** but with output in the range **[-1, 1].**
***[#yellow] <i><size:18>**Rectified Linear Unit (ReLU) -** <color #red>Widely used activation function with a "linear segment", "output" is **max(0, input).**
***[#yellow] <i><size:18>**Leaky ReLU (LReLU) -** <color #red>Variation of ReLU with a small slope for negative values. Avoids dead neurons by having a small slope for negative input.
***[#yellow] <i><size:14>**Exponential Linear Unit (ELU) -** <color #red>An activation function that avoids the vanishing gradient problem. Smoothly handles both positive and negative values.

**[#lightblue] <i>Loss Functions
***[#yellow] <i><size:14>**Mean Squared Error (MSE) -** Commonly used for **"regression"** tasks. Measures the "average squared difference" between "predicted" and "actual" values.
***[#yellow] <i><size:14>**Cross-Entropy Loss -** Ideal for **"classification"** problems. Measures the "dissimilarity" between "predicted and actual" "distributions".
***[#yellow] <i><size:14>**Hinge Loss -** Used in **"support vector machines"** and **"deep learning"**.
***[#yellow] <i><size:14>**Huber Loss -** A combination of **"MSE"** and **"absolute error loss"**.
***[#yellow] <i><size:14>**Custom Loss Functions -** Tailored for specific **tasks or objectives or problems.**

**[#lightblue] <i>Optimizers
***[#yellow] <i><size:14>**Gradient Descent -** Iterative algorithm for minimizing the loss function. Updates weights based on the gradient of the loss.
***[#yellow] <i><size:14>**Stochastic Gradient Descent (SGD) -** A "variation of gradient descent" that processes small "data batches". Uses "random samples" for faster "convergence".
***[#yellow] <i><size:14>**Adam Optimizer -** Popular adaptive "learning rate" optimizer. Adaptive optimization with "momentum".
***[#yellow] <i><size:14>**RMSprop -** Another adaptive "learning rate" **"algorithm"**. "Root Mean Square Propagation" for adjusting "learning rates".
***[#yellow] <i><size:14>**Adagrad -** Optimizer that adapts "learning rates" for each "parameter". Adaptive "learning rates" for "sparse data".

**[#lightblue] <i>Regularization
***[#yellow] <i><size:14>**L1 Regularization (Lasso) -** Encourages "sparsity" in "weight parameters".
***[#yellow] <i><size:14>**L2 Regularization (Ridge) -** Encourages "small weights" and prevents "overfitting". Prevents "large weight" values.
***[#yellow] <i><size:14>**Dropout -** Technique to prevent "overfitting" by randomly deactivating "neurons". Randomly deactivates "neurons" during "training".
***[#yellow] <i><size:14>**Batch Normalization -** Normalizes input to hidden layers, improving "training stability". Normalizes "activations" in "mini-batches".
***[#yellow] <i><size:14>**Weight Decay -** Reduces the size of "weights" during "training". "Penalizes" "large weight" values.

**[#lightblue] <i>Training and Validation
***[#yellow] <i><size:14>**Training Data and Batches -** The data used for training, divided into batches. Data used for "training" and "splitting it into batches".
***[#yellow] <i><size:14>**Learning Rate -** Controls the **"step size"** during "optimization". Controls the "step size" in "weight updates".
***[#yellow] <i><size:14>**Early Stopping -** Prevents "overfitting" by monitoring "validation loss". Prevents "overfitting" by "stopping training" when "validation error increases".
***[#yellow] <i><size:14>**Overfitting and Underfitting -** Challenges in "model" "fitting". Challenges in "finding" the "right model complexity".
***[#yellow] <i><size:14>**Validation and Testing -** Ensures model generalization and evaluation. Crucial for "assessing" "model performance".

**[#lightblue] <i>Applications
***[#yellow] <i><size:14>**Image Classification -** Identifying objects or patterns in images.
***[#yellow] <i><size:14>**Natural Language Processing (NLP) -** Processing and understanding text data.
***[#yellow] <i><size:14>**Speech Recognition -** Converting spoken language into text.
***[#yellow] <i><size:14>**Computer Vision -** Processing and understanding visual data. Analyzing visual data from images and videos.
***[#yellow] <i><size:14>**Autonomous Vehicles -** Enabling self-driving cars and robotics.

**[#lightblue] <i>Frameworks and Libraries
***[#yellow] <i><size:14>**TensorFlow -** An **open-source** and versatile **"machine learning framework"** by **Google**.
***[#yellow] <i><size:14>**PyTorch -** An open-source **"deep learning platform"** by **Facebook**. Platform for research and development.
***[#yellow] <i><size:14>**Keras -** A high-level neural networks API for rapid prototyping.
***[#yellow] <i><size:14>**scikit-learn -** A machine learning library for Python.
***[#yellow] <i><size:14>**Caffe -** A "deep learning framework" **for vision tasks** developed by the "Berkeley Vision and Learning Center".

**[#lightblue] <i>Challenges and Advanced Concepts
***[#yellow] <i><size:14>**Vanishing and Exploding Gradients -** Issues with gradients during training.
***[#yellow] <i><size:14>**Gradient Clipping -** Technique to mitigate gradient issues.
***[#yellow] <i><size:14>**Transfer Learning -** Leveraging pre-trained models for new tasks.
***[#yellow] <i><size:14>**Generative Adversarial Networks (GANs) -** Models for generating data.
***[#yellow] <i><size:14>**Reinforcement Learning with Neural Networks -** Integrating neural networks into reinforcement learning.

**[#lightblue] <i>Interdisciplinary Fields
***[#yellow] <i><size:14>**Artificial Intelligence (AI) -** "Neural networks" are a "key component" of AI. "Neural networks" play a "central role" in AI.
***[#yellow] <i><size:14>**Machine Learning (ML) -** A subset of AI that heavily relies on neural networks.
***[#yellow] <i><size:14>**Deep Learning -** A subfield of ML focused on neural networks.
***[#yellow] <i><size:14>**Cognitive Computing -** Emulating human-like thinking and reasoning.
***[#yellow] <i><size:18><color #red>**Neural Network Hardware -** Specialized hardware for efficient neural network execution.

**[#lightblue] <i>Ethical and Legal Considerations
***[#yellow] <i><size:14>**Bias and Fairness -** Addressing biases in training data and models.
***[#yellow] <i><size:14>**Privacy Concerns -** Protecting sensitive data in AI applications.
***[#yellow] <i><size:14>**Regulatory Compliance -** Adhering to legal and ethical standards.
***[#yellow] <i><size:14>**Responsible AI -** Ensuring AI systems make ethical decisions.

**[#lightblue] <i>Future Developments
***[#yellow] <i><size:14>**Architectural Innovations -** Ongoing advancements in "neural network design".
***[#yellow] <i><size:18><color #red>**Explainable AI (XAI) -** Making "AI decision-making" more "transparent".
***[#yellow] <i><size:14>**Human-Machine Collaboration -** Synergy between AI and human intelligence.
***[#yellow] <i><size:14>**Integration with IoT -** Neural networks in the Internet of Things **(IoT)**.
***[#yellow] <i><size:14>**Quantum Computing and Neural Networks -** Potential synergy between "quantum computing" and "deep learning".

**[#lightblue] <i>Research Areas and Unsolved Problems
***[#yellow] <i><size:14>**Explainability in Deep Learning -** Making neural network decisions "understandable".
***[#yellow] <i><size:14>**Compositional and Symbolic Reasoning -** Enhancing AI's "reasoning capabilities".
***[#yellow] <i><size:14>**AI Safety and Robustness -** Ensuring AI systems are safe and reliable.
***[#yellow] <i><size:14>**Generalization in Neural Networks -** Understanding and improving **"generalization".**
***[#yellow] <i><size:14>**Conscious AI and Cognitive Science -** Exploring AI with "human-like cognitive abilities".
@endmindmap
