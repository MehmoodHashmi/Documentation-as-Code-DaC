@startmindmap
title =<b><i><u>Weights and Biases
!theme hacker


*[#darkblue] <i>Weights and Biases
**[#lightblue] <i><size:14>Overview
***[#green] <i><b><color #white><size:14>Weights and biases are essential parameters in artificial neural networks, playing a critical role in modeling complex relationships in data.

**[#lightblue] <i><size:14>Weights
***[#green] <i><b><color #white><size:14>Purpose
****[#yellow] <color #black><i><size:14>Weights determine the strength of connections between neurons in a neural network.
***[#green] <i><b><color #white><size:14>Initialization
****[#yellow] <color #black><i><size:14>Weights are typically initialized with small random values before training.
***[#green] <i><b><color #white><size:14>Training
****[#yellow] <color #black><i><size:14>During training, weights are adjusted to minimize prediction errors through optimization techniques like gradient descent.
***[#green] <i><b><color #white><size:14>Weight Regularization
****[#yellow] <color #black><i><size:14>Techniques like L1 and L2 regularization are used to control the magnitude of weights and prevent overfitting.

**[#lightblue] <i><size:14>Biases
***[#green] <i><b><color #white><size:14>Role
****[#yellow] <color #black><i><size:14>Biases provide neurons with an additional degree of freedom by allowing them to shift their activation function.
***[#green] <i><b><color #white><size:14>Initialization
****[#yellow] <color #black><i><size:14>Biases are often initialized with small constants.
***[#green] <i><b><color #white><size:14>Training
****[#yellow] <color #black><i><size:14>Biases are learned alongside weights during the training process to improve model performance.

**[#lightblue] <i><size:14>Training Process
***[#green] <i><b><color #white><size:14>Forward Pass
****[#yellow] <color #black><i><size:14>Input data is propagated through the network, and predictions are made based on the weighted sum of inputs.
***[#green] <i><b><color #white><size:14>Activation
****[#yellow] <color #black><i><size:14>Biases are added to the weighted sum to shift the neuron's activation function.
***[#green] <i><b><color #white><size:14>Error Calculation
****[#yellow] <color #black><i><size:14>The difference between predictions and actual values is quantified using a loss function.
***[#green] <i><b><color #white><size:14>Backward Pass
****[#yellow] <color #black><i><size:14>Gradients are computed during backpropagation, and weight and bias updates are determined.
***[#green] <i><b><color #white><size:14>Weight and Bias Update
****[#yellow] <color #black><i><size:14>Weights and biases are updated using optimization algorithms to minimize the loss function.
***[#green] <i><b><color #white><size:14>Iteration
****[#yellow] <color #black><i><size:14>The training process is repeated for multiple epochs to improve model performance.

**[#lightblue] <i><size:14>Importance
***[#green] <i><b><color #white><size:14>Model Learning
****[#yellow] <color #black><i><size:14>Weights and biases govern how a neural network learns from data and makes predictions.
***[#green] <i><b><color #white><size:14>Model Flexibility
****[#yellow] <color #black><i><size:14>Properly tuned weights and biases allow networks to model complex functions.
***[#green] <i><b><color #white><size:14>Generalization
****[#yellow] <color #black><i><size:14>The aim is to find weight and bias values that lead to good generalization to new, unseen data.

**[#lightblue] <i><size:14>Challenges
***[#green] <i><b><color #white><size:14>Overfitting
****[#yellow] <color #black><i><size:14>Fine-tuning weights and biases to prevent overfitting is crucial.
***[#green] <i><b><color #white><size:14>Hyperparameter Tuning
****[#yellow] <color #black><i><size:14>Choosing appropriate learning rates and regularization strengths can be challenging.
***[#green] <i><b><color #white><size:14>Local Minima
****[#yellow] <color #black><i><size:14>Optimization algorithms may encounter local minima during training.

**[#lightblue] <i><size:14>Customization
***[#green] <i><b><color #white><size:14>Custom Weight Initialization
****[#yellow] <color #black><i><size:14>Specialized weight initialization strategies are employed for specific tasks.
***[#green] <i><b><color #white><size:14>Custom Activation Functions
****[#yellow] <color #black><i><size:14>Custom activation functions can be designed to suit specific modeling requirements.

**[#lightblue] <i><size:14>Deep Learning
***[#green] <i><b><color #white><size:14>Neural Network Training
****[#yellow] <color #black><i><size:14>Weights and biases play a pivotal role in training deep neural networks for tasks like image classification, natural language processing, and more.

@endmindmap
